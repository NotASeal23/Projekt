---
title: "Ćwiczenie 6"
format: 
  html:
    self-contained: true
    theme:
      light: cosmo
      dark: darkly
    fontsize: 1.0em
    toc: true
    toc-location: left
    toc-title: "Spis tresci"
    number-sections: true
    code-fold: show 
    code-summary: "Ukryj/Pokaz kod"
    code-tools: true
    code-block-bg: "lightgrey"
    code-block-border-left: "black" 
    code-line-numbers: true
    code-copy: false
editor: visual
author: "Gabriel Rączkowski 416529"
editor_options: 
  chunk_output_type: console
execute:
  warning: false
  echo: true
  error: false
---

```{r}
#| message: false
#| warning: false
#| echo: false
#include
library(tidymodels) 
library(skimr) 
library(GGally) 
library(openair) 
library(DT) 
library(dplyr)
library(ranger)
tidymodels_prefer()
library(tidymodels)
library(rpart)
library(glmnet)
library(rpart.plot)  
library(vip)        
```

### Data preparation

```{r}
#moje dane 
air <- mydata |> selectByDate(year = 2004)
air <-
    air |> na.omit()

corcc <-
  air |>
  as_tibble() |>
  na.omit() |> 
  select(-c(date)) 
cor(corcc) # sprawdzam korelacje


air <- 
    air |> mutate(
        wd = cut(
            wd,
            labels = seq(1, 16),
            breaks = 16
            
        )
    )

set.seed(222)
data_split <- initial_split(data = air, prop = 3/4, strata = o3)
train_data <- training(data_split)
test_data <-  testing(data_split)


```


### Models and grids

```{r}
#rand_forest
#decision_tree
#linear_reg




dt_ts =
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune(),
    min_n = tune()
  ) |> 
  set_engine("rpart") |> 
  set_mode("regression")

rf_ts = 
  rand_forest(
    mtry = tune(),
    trees = tune(),
    min_n = tune()
  ) |> 
  set_engine(
    engine = "ranger"
  ) |> 
  set_mode("regression")
  
lr_ts = 
  linear_reg(
    penalty = tune(),
    mixture = tune()
  ) |> 
  set_engine(
    engine = "glmnet"
  ) |> 
  set_mode("regression")

lr_grid <-
    grid_regular(
        penalty(),
        mixture(),
        levels = 3
    )

dt_grid <-
    grid_regular(
        cost_complexity(),
        tree_depth(),
        min_n(),
        levels = 2
    )

rf_grid <-
    grid_regular(
        mtry(range=c(1, 8)),
        trees(),
        min_n(),
        levels = 2
    )
```  


### Recipes

```{r}  


#nie uzywamy zmiennych, ktore sa silnie skorelowane
air_recipe <-
  recipe(o3 ~ ., data = train_data) |>
  update_role(date,nox,no2,pm10, new_role = "ID") |>  
  step_date(date, features = c("month"))  |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors())

workf_lr  <- 
  workflow() |> 
  add_model(lr_ts) |> 
  add_recipe(air_recipe)

workf_rf  <- 
  workflow() |> 
  add_model(rf_ts) |> 
  add_recipe(air_recipe)

workf_dt  <- 
  workflow() |> 
  add_model(dt_ts) |> 
  add_recipe(air_recipe)


```
### Fitting data

```{r}
folds <- vfold_cv(train_data)



fit_rf <-
  workf_rf |>
  tune_grid(
    resamples = folds,
    grid = rf_grid,
    metrics = yardstick::metric_set(mae)
  )


fit_dt <-
  workf_dt |>
  tune_grid(
    resamples = folds,
    grid = dt_grid,
    metrics = yardstick::metric_set(mae)
  )

fit_lr <-
  workf_lr |>
  tune_grid(
    resamples = folds,
    grid = lr_grid,
    metrics = yardstick::metric_set(mae)
  )


```

### Selekcja najlepszego modelu
```{r}

lr_model <- select_best(fit_lr, metric ="mae")
dt_model <- select_best(fit_dt, metric ="mae")
rf_model <- select_best(fit_rf, metric ="mae") #

lr_bmod <-
    workf_lr |>
    finalize_workflow(lr_model)

dt_bmod <-
    workf_dt |>
    finalize_workflow(dt_model)

rf_bmod <-
    workf_rf |>
    finalize_workflow(rf_model)
```




```{r}
final_lr <- finalize_model(lr_ts, lr_model)
final_dt <- finalize_model(dt_ts, dt_model)
final_rf <- finalize_model(lr_ts, rf_model)


final_lr_wf <- workflow() |> 
  add_model(final_lr) |> 
  add_recipe(air_recipe)

final_dt_wf <- workflow() |> 
  add_model(final_dt) |> 
  add_recipe(air_recipe)

final_rf_wf <- workflow() |> 
  add_model(final_rf) |> 
  add_recipe(air_recipe)




lr_ffit <- final_lr_wf |> 
  fit(data = train_data)

dt_ffit <- final_dt_wf |> 
  fit(data = train_data)

rf_ffit <- final_rf_wf |> 
  fit(data = train_data)

preds_lr <- lr_ffit |> predict(test_data) |> bind_cols(test_data)
preds_dt <- dt_ffit |> predict(test_data) |> bind_cols(test_data)
preds_rf <- rf_ffit |> predict(test_data) |> bind_cols(test_data)



lr_metrics <- preds_lr |> 
  metrics(truth = o3, estimate = .pred)

dt_metrics <- preds_dt |> 
  metrics(truth = o3, estimate = .pred)

rf_metrics <- preds_rf |> 
  metrics(truth = o3, estimate = .pred)

lr_metrics |> print()
dt_metrics |> print()
rf_metrics |> print()
#najlepszą metrykę mae ma model drzewa decyzyjnego z parametrami:
print(dt_model)
```